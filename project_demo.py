# -*- coding: utf-8 -*-
"""project_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zogQXuY4iicT7Wo9P8W9Up-aOXCnEFIz

Import content image and style image from google drive
"""

# the first part of the code
# is setting up the path to load
# the content and style images 
# stored in our shared google drive

!pip install PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Get the image file into workplace from drive
downloaded_content = drive.CreateFile({'id':"1JkY5DsJX_YQ3MjVIT9HiYVYieyY4jNsU"})   # replace the id with id of file you want to access
downloaded_content.GetContentFile('NYC.jpg')
# how to get the file id see
# https://buomsoo-kim.github.io/colab/2018/04/16/Importing-files-from-Google-Drive-in-Google-Colab.md/

downloaded_style = drive.CreateFile({'id':"1rDOFUgM-_hpaamRFEhdrwD7gqb2delaB"}) 
downloaded_style.GetContentFile('star.jpg')

# Get the image file into workplace from drive
downloaded_content = drive.CreateFile({'id':"17sqR_nRghp_c5K23-_SSl6kfpx-btEJ2"})   # replace the id with id of file you want to access
downloaded_content.GetContentFile('Shanghai.jpg')
# how to get the file id see
# https://buomsoo-kim.github.io/colab/2018/04/16/Importing-files-from-Google-Drive-in-Google-Colab.md/

downloaded_style = drive.CreateFile({'id':"1g0ghMll_smMYTmGpH70WsjqqaiMnwOPO"}) 
downloaded_style.GetContentFile('painting.jpg')

downloaded_content = drive.CreateFile({'id':"17dAlQ-0qDZHu8_Ll3s-VMsvdN2q08xIp"})   # replace the id with id of file you want to access
downloaded_content.GetContentFile('qibaishi.jpg')

downloaded_content = drive.CreateFile({'id':"1wzsVYEXQFEYthDjhposjt4BkNXL_HCNH"})   # replace the id with id of file you want to access
downloaded_content.GetContentFile('ide.jpg')

downloaded_content = drive.CreateFile({'id':"1g1XKBQ4mgO56WFU682zPBLESY9Udqgte"})   # replace the id with id of file you want to access
downloaded_content.GetContentFile('painting_1.jpg')

downloaded_content = drive.CreateFile({'id':"1uRRiT3UL9N4SNLqhacjSSRwOmPGcGqSN"})  
downloaded_content.GetContentFile('van_2.jpg')

downloaded_content = drive.CreateFile({'id':"1p7KDTNu6LCy3yfNmklXqrfgHTCtSX26K"})  
downloaded_content.GetContentFile('boom.jpg')

downloaded_content = drive.CreateFile({'id':"1s0OC2ORXGAoIj8Av4z4VEggc55B34s9B"})  
downloaded_content.GetContentFile('sumiao.jpg')

downloaded_content = drive.CreateFile({'id':"1TiTU2XAfToc_3ifEHzCorwn0ejke3wfN"})  
downloaded_content.GetContentFile('mao.jpg')

downloaded_content = drive.CreateFile({'id':"140Jy0cyq0sT6ins5CuIr8mSKS7VmLKrY"})  
downloaded_content.GetContentFile('kaishu.jpg') 

downloaded_content = drive.CreateFile({'id':"144O77EIb-5HGw6J5AyEuXhaAI2ph1h6S"})  
downloaded_content.GetContentFile('house1.jpg')

downloaded_content = drive.CreateFile({'id':"1Iv-KPweaK3EX3HsLZQrrlYwzuEgIx0ry"})  
downloaded_content.GetContentFile('win_10.jpg')

downloaded_content = drive.CreateFile({'id':"1LhwlQBaaDy_n-RGA3pg8Ybu5D1Jt2XVP"})  
downloaded_content.GetContentFile('huohua_1.jpg')

downloaded_content = drive.CreateFile({'id':"1XxB783j2jgYXcq0zujFriqI319WBt-OP"})  
downloaded_content.GetContentFile('jg.jpg')

downloaded_content = drive.CreateFile({'id':"1UGymYEWUgKflNTyrXNkQ_u3_JDQAYbTk"})  
downloaded_content.GetContentFile('trump.jpg')

downloaded_content = drive.CreateFile({'id':"14R5eDTv1FAHwDU7h4fcmtW8xQB_ecTnk"})  
downloaded_content.GetContentFile('chip.jpg')

downloaded_content = drive.CreateFile({'id':"1HuIG371pTdvBXGg3CqOPQc9XwGmwyTs7"})  
downloaded_content.GetContentFile('math.jpg')

downloaded_content = drive.CreateFile({'id':"1V5OMzSHXOB-59aAI4Tthh4Svr0iI_3bi"})  
downloaded_content.GetContentFile('chem.jpg')

downloaded_content = drive.CreateFile({'id':"1SBIAmqpLWbprKrORFuv-sKjy1nYCo_4S"})  
downloaded_content.GetContentFile('math2.jpg')

downloaded_content = drive.CreateFile({'id':"1Mjrpwhbci4R8FdNP5FRrpajst7m4nxWN"})  
downloaded_content.GetContentFile('math3.jpg')

downloaded_content = drive.CreateFile({'id':"1KB81-66AeFEYonjwgkPo_3Y9d7yjdATk"})  
downloaded_content.GetContentFile('math4.jpg')

downloaded_content = drive.CreateFile({'id':"1BX4xxf8fZs_kMwcoZuq6xhDmJwuAzURW"})  
downloaded_content.GetContentFile('fgo1.jpg')

downloaded_content = drive.CreateFile({'id':"1b7_KlNEprRAfvCoKHsBJaxcEdSiX8rnh"})  
downloaded_content.GetContentFile('bingmayong.jpg')

"""Import and configure modules"""

# main part of the following code are from the tensorFlow tutorial
# https://www.tensorflow.org/tutorials/generative/style_transfer
# we did the following modification

import os
import tensorflow as tf
# Load compressed models from tensorflow_hub
os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'

import IPython.display as display

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12,12)
mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image
import time
import functools

def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

"""Visualize the input"""

import tensorflow as tf
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from keras.applications.vgg16 import VGG16
import time

# functions to show images

# Define a function to load an image and limit ites maximum dimension to 512 pixels

def load_img(path_to_img):
  max_dim = 512
  img = tf.io.read_file(path_to_img)
  img = tf.image.decode_image(img, channels=3)
  img = tf.image.convert_image_dtype(img, tf.float32)

  shape = tf.cast(tf.shape(img)[:-1], tf.float32)
  long_dim = max(shape)
  scale = max_dim / long_dim

  new_shape = tf.cast(shape * scale, tf.int32)

  img = tf.image.resize(img, new_shape)
  img = img[tf.newaxis, :]
  return img

# Create a simple function to display an image
def imshow(image, title=None):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)

  plt.imshow(image)
  if title:
    plt.title(title)

height=512
width=512

# the second sharpen method
from PIL import Image
from PIL import ImageFilter

def sharpen_2(img):
  imageObject = tensor_to_image(img)

  sharpened1 = imageObject.filter(ImageFilter.SHARPEN)
  sharpened2 = sharpened1.filter(ImageFilter.SHARPEN)  

  sharpend1_array = tf.keras.preprocessing.image.img_to_array(sharpened1)/255

  max_dim = 512
  shape = tf.cast(tf.shape(sharpend1_array)[:-1], tf.float32)
  long_dim = max(shape)
  scale = max_dim / long_dim

  new_shape = tf.cast(shape * scale, tf.int32)
  sharpend1_tf = tf.image.resize(sharpend1_array, new_shape)
  sharpend1_tf = sharpend1_tf[tf.newaxis, :]

  return sharpend1_tf

# preprocess the content image and style image
content_image_ori = load_img('bingmayong.jpg')
style_image_ori = load_img('chip.jpg')

content_image = content_image_ori
content_image = tf.image.rgb_to_grayscale(content_image_ori)
content_image = tf.image.grayscale_to_rgb(content_image)
content_image = sharpen_2(content_image)
imshow(content_image)

style_image = style_image_ori
#style_image = tf.image.rgb_to_grayscale(style_image_ori)
#style_image = tf.image.grayscale_to_rgb(style_image)
style_image = sharpen_2(style_image)
imshow(style_image)

# manually building VGG_19 
def VGG_19(input_shape):

  image_input = Input(shape=input_shape)

  # Block 1
  x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same',name='block1_conv1')(image_input)
  x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same',name='block1_conv2')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block1_pool')(x)

  #Block 2
  x = Conv2D(filters=128, kernel_size=(3,3),activation='relu', padding='same',name='block2_conv1')(x)
  x = Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same',name='block2_conv2')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block2_pool')(x)

  #Block 3
  x = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding='same',name='block3_conv1')(x)
  x = Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same',name='block3_conv2')(x)
  x = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding='same',name='block3_conv3')(x)
  x = Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same',name='block3_conv4')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block3_pool')(x)

  #Block 4
  x = Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same',name='block4_conv1')(x)
  x = Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same',name='block4_conv2')(x)
  x = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding='same',name='block4_conv3')(x)
  x = Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same',name='block4_conv4')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block4_pool')(x)

  #Block 5 same as Block 4 for the conv and pooling
  x = Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same',name='block5_conv1')(x)
  x = Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same',name='block5_conv2')(x)
  x = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding='same',name='block5_conv3')(x)
  x = Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same',name='block5_conv4')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block5_pool')(x)

  x = Flatten(name = 'flatten')(x)
  x = Dense(4096, activation='relu', name='fc1')(x)
  x = Dense(4096, activation='relu', name='fc2')(x)
  x = Dense(1000, activation='softmax', name='predictions')(x)

  return x

"""Define content and style representations"""

input_shape = (512, 512, 3)
#x = tf.keras.applications.vgg16.preprocess_input(content_image*255)
#x = tf.image.resize(x, (224, 224))
x = tf.image.resize(content_image, (224, 224))
# to get the VGG16 model from Keras
vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')
# to get the VGG19 model from Keras
# vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')

prediction_probabilities = vgg(x)
prediction_probabilities.shape

vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')

print()
for layer in vgg.layers:
  print(layer.name)

vgg.summary()

# choose intermediate layers from the network to
# represent the style and content of the image:

# VGG 19

"""
# we select the content layers and style layers follow the 
# procedure from the paper
# select block4 conv 2 as content layer
content_layers = ['block5_conv2'] 

# select the following layers as style layer 
style_layers = ['block1_conv1',
          'block2_conv1',
          'block3_conv1', 
          'block4_conv1', 
          'block5_conv1']
"""

"""
# select the following layers as style layer 
style_layers = ['block4_conv1',
          'block5_conv1',]
"""

content_layers = ['block2_conv2'] 
style_layers = ['block3_conv1','block4_conv1']

# shumiao, block3 conv1 is the best
# usually content layer block1 conv2; style layer:block 3 conv1 
# trump + chip content: b5c2, style: b2c1,b3c1,b4c1

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)
num_content_layers, num_style_layers

"""Build the model"""

def vgg_layers(layer_names):
  """ Creates a vgg model that returns a list of intermediate output values."""
  # Load our model. Load pretrained VGG, trained on imagenet data
  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
  vgg.trainable = False

  outputs = [vgg.get_layer(name).output for name in layer_names]

  model = tf.keras.Model([vgg.input], outputs)
  return model

style_extractor = vgg_layers(style_layers)
style_outputs = style_extractor(style_image*255)

#Look at the statistics of each layer's output
for name, output in zip(style_layers, style_outputs):
  print(name)
  print("  shape: ", output.numpy().shape)
  print("  min: ", output.numpy().min())
  print("  max: ", output.numpy().max())
  print("  mean: ", output.numpy().mean())
  print()

"""Calculate style"""

# we modified the gram matrix
def gram_matrix(input_tensor):
  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
  input_shape = tf.shape(input_tensor)
  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
  # mean of sum_k F^l_{ik} F^l_{jk}, used in the tutorial
  # sum_k F^l_{ik} F^l_{jk} / (I J)
  mean_sum = result/(num_locations)
  # sum_k F^l_{ik} F^l_{jk}, used in the paper
  sum = result
  return result

"""Extract style and content
* build a model that returns the style and content tensors.
"""

# the class object of style content model
class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(StyleContentModel, self).__init__()
    self.vgg =  vgg_layers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False

  def call(self, inputs):
    "Expects float input in [0,1]"
    inputs = inputs*255.0
    preprocessed_input = tf.keras.applications.vgg16.preprocess_input(inputs)
    outputs = self.vgg(preprocessed_input)
    style_outputs, content_outputs = (outputs[:self.num_style_layers], 
                        outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output)
              for style_output in style_outputs]

    content_dict = {content_name:value 
              for content_name, value 
              in zip(self.content_layers, content_outputs)}

    style_dict = {style_name:value
            for style_name, value
            in zip(self.style_layers, style_outputs)}

    return {'content':content_dict, 'style':style_dict}

# to create a style content model object
extractor = StyleContentModel(style_layers, content_layers)

results = extractor(tf.constant(content_image))

print('Styles:')
for name, output in sorted(results['style'].items()):
  print("  ", name)
  print("    shape: ", output.numpy().shape)
  print("    min: ", output.numpy().min())
  print("    max: ", output.numpy().max())
  print("    mean: ", output.numpy().mean())
  print()

print("Contents:")
for name, output in sorted(results['content'].items()):
  print("  ", name)
  print("    shape: ", output.numpy().shape)
  print("    min: ", output.numpy().min())
  print("    max: ", output.numpy().max())
  print("    mean: ", output.numpy().mean())

"""Run gradient descent"""

# setting the style and content target values as following
style_targets = extractor(style_image)['style']
content_targets = extractor(content_image)['content']

# modified the initial image to optimize
# the tutorial uses the content image as the 
# initial. We first set it to be the 
# white noise as the paper instructed
# image = tf.Variable(content_image)

# to generate white noise images
# using the normal distribution with
# mean = 0, sd = 0.5
shape = tf.cast(tf.shape(content_image)[:], tf.int32)

white_noise_image = np.random.normal(0, 0.5, size = shape)

# bluring function from
# https://gist.github.com/blzq/c87d42f45a8c5a53f5b393e27b1f5319
# and we did some modify on the return value

def gaussian_blur(img, kernel_size=11, sigma=5):
    def gauss_kernel(channels, kernel_size, sigma):
        ax = tf.range(-kernel_size // 2 + 1.0, kernel_size // 2 + 1.0)
        xx, yy = tf.meshgrid(ax, ax)
        kernel = tf.exp(-(xx ** 2 + yy ** 2) / (2.0 * sigma ** 2))
        kernel = kernel / tf.reduce_sum(kernel)
        kernel = tf.tile(kernel[..., tf.newaxis], [1, 1, channels])
        return kernel

    gaussian_kernel = gauss_kernel(tf.shape(img)[-1], kernel_size, sigma)
    gaussian_kernel = gaussian_kernel[..., tf.newaxis]

    return tf.nn.depthwise_conv2d(img, gaussian_kernel, [1, 1, 1, 1],
                                  padding='SAME', data_format='NHWC')

# ini with white noise
#white_noise_image_tf = tf.convert_to_tensor(white_noise_image, dtype=tf.float32)
#image = tf.Variable(white_noise_image_tf)

# ini with content image
image = tf.Variable(content_image)

# ini with blur content image
#blur_ini = gaussian_blur(content_image, kernel_size=15, sigma=5)
#image = tf.Variable(blur_ini)

tensor_to_image(image)

def clip_0_1(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)
# follows the setting in the paper, both are set to 0.5
#style_weight=0.5
#content_weight=0.5

style_weight=1e-3
content_weight=1e4
variation_wight=30

# modified by adding a new loss: total variation loss
# this idea was from https://towardsdatascience.com/practical-techniques-for-getting-style-transfer-to-work-19884a0d69eb
# the total variation loss is the summ of the absolute differences for neighboring pixel
# values in the input images. 
# adding total variation loss to the training loss removes
# the rough texture of the image and the resultant images
# looks much smoother

def total_loss(outputs, alpha = 10000, beta = 0.1,style_W = style_weight,
         content_W = content_weight, variation_W = variation_wight):
    style_outputs = outputs['style']
    content_outputs = outputs['content']
    style_loss = tf.add_n([tf.reduce_sum((style_outputs[name]-style_targets[name])**2) 
                           for name in style_outputs.keys()])
    style_loss *= style_weight / num_style_layers


    content_loss = tf.add_n([tf.reduce_sum((content_outputs[name]-content_targets[name])**2) 
                             for name in content_outputs.keys()])
    content_loss *= content_weight / num_content_layers

    variation_loss = tf.image.total_variation(output)
    variation_loss *= variation_W


    loss = beta*style_loss + alpha*content_loss + variation_loss
    
    return loss

@tf.function()
def train_step(image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = total_loss(outputs, alpha=100, beta = 0.1) # alpha = 10000, beta = 0.1 for content ini

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, image)])
  image.assign(clip_0_1(image))
# shanghai + star, alpha 100, beta 0.1, layer: content b1conv2, style: b1conv1, b3conv1, b4conv1

import IPython.display as display
import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12,12)
mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image
import time
import functools
def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

from PIL import Image
from PIL import ImageFilter

# Open an already existing image
imageObject = tensor_to_image(content_image);
imageObject.show();

# Apply sharp filter
sharpened1 = imageObject.filter(ImageFilter.SHARPEN);
sharpened2 = sharpened1.filter(ImageFilter.SHARPEN);

# Show the sharpened images
#sharpened1 = sharpened1.resize((512,512))
#sharpened2 = sharpened2.resize((512,512))

sharpend1_array = tf.keras.preprocessing.image.img_to_array(sharpened1)/255

max_dim = 512
shape = tf.cast(tf.shape(sharpend1_array)[:-1], tf.float32)
long_dim = max(shape)
scale = max_dim / long_dim

new_shape = tf.cast(shape * scale, tf.int32)
new_shape

#sharpend1_array.resize(1, 512, 512, 3)
#sharpend1_tf = tf.convert_to_tensor(sharpend1_array, dtype=tf.float32)
#tensor_to_image(sharpend1_tf)

img = tf.image.resize(sharpend1_array, new_shape)
img = img[tf.newaxis, :]

# the second sharpen method
from PIL import Image
from PIL import ImageFilter

def sharpen_2(img):
  imageObject = tensor_to_image(img)

  sharpened1 = imageObject.filter(ImageFilter.SHARPEN)
  sharpened2 = sharpened1.filter(ImageFilter.SHARPEN)  

  sharpend1_array = tf.keras.preprocessing.image.img_to_array(sharpened1)/255

  max_dim = 512
  shape = tf.cast(tf.shape(sharpend1_array)[:-1], tf.float32)
  long_dim = max(shape)
  scale = max_dim / long_dim

  new_shape = tf.cast(shape * scale, tf.int32)
  sharpend1_tf = tf.image.resize(sharpend1_array, new_shape)
  sharpend1_tf = sharpend1_tf[tf.newaxis, :]

  return sharpend1_tf

import tensorflow_datasets as tfds
train_step(image)

tensor_to_image(image)

import time
start = time.time()

epochs = 1*50
steps_per_epoch = 50

step = 0
for n in range(epochs):
  for m in range(steps_per_epoch):
    step += 1
    train_step(image)
    print(".", end='')
  display.clear_output(wait=True)
  # adding a blur process after each epoch, you can comment it out if needed
  #image.assign(gaussian_blur(image, kernel_size= 2.5, sigma=5))
  # to sharpen the output image
  #image.assign(sharpen_2(image))
  # force to be black and white
  #image.assign(tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image)))
  display.display(tensor_to_image(image))
  print("Train step: {}".format(step))

end = time.time()
print("Total time: {:.1f}".format(end-start))

"""Report Summary：
* Introduction
  - Background
  - Goal
* Methods
  - Model
  - Loss function
* Dataset and processing
  - Dataset information
  - processing
* Result
  - sample
  - result
* Summary
  
* Reference

听了老师课上关于report的要求&老师对project4的想法和理解：

length:
4-8页

* abstract
  - why this project interest, why do this, important insight

* intro 
 - define problem, what u do &  found
* method
 - explain what u do, equation, how to build, core insight 
*result
 - demonstrate what you do (figure, train&test )

compare baseline(optional)

老师对project4的理解/侧重点: practical application&inference&result
"""