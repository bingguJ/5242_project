# -*- coding: utf-8 -*-
"""“project_demo.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DMUPpfFlbOMc6rC8s97yC3f3V1_7ca8K

Import content image and style image from google drive
"""

!pip install PyDrive

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Get the image file into workplace from drive
downloaded_content = drive.CreateFile({'id':"1x_8AOpYleVK5KP0_v42Fp-jteL5huBbX"})   # replace the id with id of file you want to access
downloaded_content.GetContentFile('Shanghai.jpg')
# how to get the file id see
# https://buomsoo-kim.github.io/colab/2018/04/16/Importing-files-from-Google-Drive-in-Google-Colab.md/

downloaded_style = drive.CreateFile({'id':"1whwsS_jBukbK9r1PiBXM7248vutfF3L1"}) 
downloaded_style.GetContentFile('painting.jpg')

# Get the image file into workplace from drive
downloaded_content = drive.CreateFile({'id':"1JkY5DsJX_YQ3MjVIT9HiYVYieyY4jNsU"})   # replace the id with id of file you want to access
downloaded_content.GetContentFile('NYC.jpg')
# how to get the file id see
# https://buomsoo-kim.github.io/colab/2018/04/16/Importing-files-from-Google-Drive-in-Google-Colab.md/

downloaded_style = drive.CreateFile({'id':"1rDOFUgM-_hpaamRFEhdrwD7gqb2delaB"}) 
downloaded_style.GetContentFile('star.jpg')

"""Import and configure modules"""

import os
import tensorflow as tf
# Load compressed models from tensorflow_hub
os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'

import IPython.display as display

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12,12)
mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image
import time
import functools

def tensor_to_image(tensor):
  tensor = tensor*255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

"""Visualize the input"""

import tensorflow as tf
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
from keras.applications.vgg16 import VGG16
import time

# functions to show images

# Define a function to load an image and limit ites maximum dimension to 512 pixels
def vgg_input_resize(path_to_img,new_width=244,new_height=244):
  img = PIL.Image.open(path_to_img)
  img = np.expand_dims(np.array(img.resize((new_width,new_height))),axis = 0)
  #plt.imshow(style_image)
  return img

def load_img(path_to_img,max_dim = 512):
  img = PIL.Image.open(path_to_img)
  width,height = img.size
  new_width,new_height = 0,0
  if width >= height:
    new_width = max_dim
    new_height = round(new_width/width*height)
  else:
    new_height = max_dim
    new_width = round(new_height/height*width)

  img = np.expand_dims(np.array(img.resize((new_width,new_height))),axis = 0)
  #plt.imshow(style_image)
  return img

# Create a simple function to display an image
def imshow(image, title=None):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)

  img = image.numpy()
  plt.imshow(image)
  if title:
    plt.title(title)

height=512
width=512

# preprocess the content image and style image
content_image = load_img('Shanghai.jpg')
style_image = load_img('painting.jpg')

content_image.resize((1,33))

imshow(content_image)

imshow(style_image)

# manually building VGG_19 
def VGG_19(input_shape):

  image_input = Input(shape=input_shape)

  # Block 1
  x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same',name='block1_conv1')(image_input)
  x = Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same',name='block1_conv2')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block1_pool')(x)

  #Block 2
  x = Conv2D(filters=128, kernel_size=(3,3),activation='relu', padding='same',name='block2_conv1')(x)
  x = Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same',name='block2_conv2')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block2_pool')(x)

  #Block 3
  x = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding='same',name='block3_conv1')(x)
  x = Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same',name='block3_conv2')(x)
  x = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding='same',name='block3_conv3')(x)
  x = Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same',name='block3_conv4')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block3_pool')(x)

  #Block 4
  x = Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same',name='block4_conv1')(x)
  x = Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same',name='block4_conv2')(x)
  x = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding='same',name='block4_conv3')(x)
  x = Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same',name='block4_conv4')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block4_pool')(x)

  #Block 5 same as Block 4 for the conv and pooling
  x = Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same',name='block5_conv1')(x)
  x = Conv2D(filters=512, kernel_size=(3,3), activation='relu', padding='same',name='block5_conv2')(x)
  x = Conv2D(filters=256, kernel_size=(3,3),activation='relu', padding='same',name='block5_conv3')(x)
  x = Conv2D(filters=256, kernel_size=(3,3), activation='relu', padding='same',name='block5_conv4')(x)
  x = MaxPool2D((2,2), strides=(2,2), name = 'block5_pool')(x)

  x = Flatten(name = 'flatten')(x)
  x = Dense(4096, activation='relu', name='fc1')(x)
  x = Dense(4096, activation='relu', name='fc2')(x)
  x = Dense(1000, activation='softmax', name='predictions')(x)

  return x

"""Define content and style representations"""

input_shape = (512, 512, 3)
#x = tf.keras.applications.vgg16.preprocess_input(content_image*255)
#x = tf.image.resize(x, (224, 224))
# to get the VGG16 model from Keras
vgg = tf.keras.applications.VGG16(include_top=True, weights='imagenet')
# to get the VGG19 model from Keras
# vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')

vgg.input

content_image = vgg_input_resize('Shanghai.jpg')
style_image = vgg_input_resize('painting.jpg')

vgg(content_image)
#np.array(content_img_resize).shape

prediction_probabilities = vgg(content_img_resize)
prediction_probabilities.shape

predicted_top_5 = tf.keras.applications.vgg16.decode_predictions(prediction_probabilities.numpy())[0]
[(class_name, prob) for (number, class_name, prob) in predicted_top_5]

vgg = tf.keras.applications.VGG16(include_top=False, weights='imagenet')
print()
for layer in vgg.layers:
  print(layer.name)

# choose intermediate layers from the network to
# represent the style and content of the image:

# VGG 19
# select block5 conv 2 as content layer
content_layers = ['block5_conv2'] 

# select the following layers as style layer
style_layers = ['block1_conv1',
          'block2_conv1',
          'block3_conv1', 
          'block4_conv1', 
          'block5_conv1']

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

"""Build the model"""

def vgg_layers(layer_names):
  """ Creates a vgg model that returns a list of intermediate output values."""
  # Load our model. Load pretrained VGG, trained on imagenet data
  vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
  vgg.trainable = False

  outputs = [vgg.get_layer(name).output for name in layer_names]
  print(outputs)

  model = tf.keras.Model([vgg.input], outputs)
  return model

style_extractor = vgg_layers(style_layers)
style_outputs = style_extractor(style_image*255)

#Look at the statistics of each layer's output
for name, output in zip(style_layers, style_outputs):
  print(name)
  print("  shape: ", output.numpy().shape)
  print("  min: ", output.numpy().min())
  print("  max: ", output.numpy().max())
  print("  mean: ", output.numpy().mean())
  print()

"""Calculate style"""

#def gram_matrix(input_tensor):
#  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
#  input_shape = tf.shape(input_tensor)
#  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
#  return result/(num_locations)
def gram_matrix(input_tensor):
  result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)
  #input_shape = tf.shape(input_tensor)
  #num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)
  #result/(num_locations)
  return result

"""Extract style and content
* build a model that returns the style and content tensors.
"""

class StyleContentModel(tf.keras.models.Model):
  def __init__(self, style_layers, content_layers):
    super(StyleContentModel, self).__init__()
    self.vgg =  vgg_layers(style_layers + content_layers)
    self.style_layers = style_layers
    self.content_layers = content_layers
    self.num_style_layers = len(style_layers)
    self.vgg.trainable = False

  def call(self, inputs):
    "Expects float input in [0,1]"
    #inputs = inputs*255.0
    #preprocessed_input = tf.keras.applications.vgg16.preprocess_input(inputs)
    #outputs = self.vgg(preprocessed_input)
    outputs = self.vgg(inputs)
    style_outputs, content_outputs = (outputs[:self.num_style_layers], 
                        outputs[self.num_style_layers:])

    style_outputs = [gram_matrix(style_output)
              for style_output in style_outputs]

    content_dict = {content_name:value 
              for content_name, value 
              in zip(self.content_layers, content_outputs)}

    style_dict = {style_name:value
            for style_name, value
            in zip(self.style_layers, style_outputs)}

    return {'content':content_dict, 'style':style_dict}

c_255 = content_image*255

c_255[0][0][0][0]
#content_image[0][0][0][0]

type(content_image[0][0][0][0])
np.uint8(content_image[0][0][0][0]*255)

imshow(content_image)

tf.keras.applications.vgg16.preprocess_input(content_image).shape

extractor = StyleContentModel(style_layers, content_layers)

results = extractor(tf.constant(content_image))

print('Styles:')
for name, output in sorted(results['style'].items()):
  print("  ", name)
  print("    shape: ", output.numpy().shape)
  print("    min: ", output.numpy().min())
  print("    max: ", output.numpy().max())
  print("    mean: ", output.numpy().mean())
  print()

print("Contents:")
for name, output in sorted(results['content'].items()):
  print("  ", name)
  print("    shape: ", output.numpy().shape)
  print("    min: ", output.numpy().min())
  print("    max: ", output.numpy().max())
  print("    mean: ", output.numpy().mean())

"""Run gradient descent"""

style_targets = extractor(style_image)['style']
content_targets = extractor(content_image)['content']

image = tf.Variable(content_image,dtype=tf.float32)

tuple(x for x in image.shape)

plain_image = tf.Variable(np.random.normal(size=tuple(x for x in image.shape)),dtype=tf.float32)

plain_image

def clip_0_1(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)

def clip_0_255(image):
  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0)

opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)
#style_weight=1e-2
#content_weight=1e4
style_weight=1
content_weight=1

def style_content_loss(outputs):
    style_outputs = outputs['style']
    content_outputs = outputs['content']
    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) 
                           for name in style_outputs.keys()])
    style_loss *= style_weight / num_style_layers

    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) 
                             for name in content_outputs.keys()])
    content_loss *= content_weight / num_content_layers
    loss = style_loss + content_loss
    # try different loss function
    
    return loss

@tf.function()
def train_step(image,plain_image=plain_image):
  with tf.GradientTape() as tape:
    outputs = extractor(image)
    loss = style_content_loss(outputs)

  grad = tape.gradient(loss, image)
  opt.apply_gradients([(grad, plain_image)])
  #image.assign(clip_0_1(image))
  plain_image.assign(clip_0_255(plain_image))

import IPython.display as display

import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.figsize'] = (12,12)
mpl.rcParams['axes.grid'] = False

import numpy as np
import PIL.Image
import time
import functools
def tensor_to_image(tensor):
  tensor = tensor*255
  print(tensor)
  tensor = tf.convert_to_tensor(tensor)
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor)>3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return PIL.Image.fromarray(tensor)

my_out = extractor(image)
my_loss = style_content_loss(my_out)

my_loss

train_step(image)
train_step(image)
train_step(image)
tensor_to_image(plain_image)

#tensor_to_image()
#tensor_to_image(image)

#type(image)
image.shape
#np.array(image,dtype=np.float128)
#np.array(image,dtype=np.uint8)
#tensor = np.array(tf.constant(image), dtype=np.uint8)

np.array(tf.convert_to_tensor(image))

import time
start = time.time()

epochs = 20
steps_per_epoch = 10

step = 0
for n in range(epochs):
  for m in range(steps_per_epoch):
    step += 1
    train_step(image)
    print(".", end='')
  display.clear_output(wait=True)
  display.display(tensor_to_image(image))
  print("Train step: {}".format(step))

end = time.time()
print("Total time: {:.1f}".format(end-start))

"""Report Summary：
* Introduction
  - Background
  - Goal
* Methods
  - Model
  - Loss function
* Dataset and processing
  - Dataset information
  - processing
* Result
  - sample
  - result
* Summary
  
* Reference

听了老师课上关于report的要求&老师对project4的想法和理解：

length:
4-8页

* abstract
  - why this project interest, why do this, important insight

* intro 
 - define problem, what u do &  found
* method
 - explain what u do, equation, how to build, core insight 
*result
 - demonstrate what you do (figure, train&test )

compare baseline(optional)

老师对project4的理解/侧重点: practical application&inference&result
"""

